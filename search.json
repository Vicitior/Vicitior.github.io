[{"title":"实验室暑期考核","url":"/posts/85335216/","content":"第一题\n通过分析题意，我们不难看出，这是一个十分经典的算法，经典到上学期的一个重点就是对二叉树进行一个前中后序的遍历。\n\n根据我们的知识，无非就是运用递归算法，一直寻找左子树，直达寻找到叶子节点，将它存放到栈中，再把它的父节点存放到栈中，之后再寻找父节点右子树的左子树。\n代码：\nclass Solution &#123;\npublic:\n  vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123;\n        vector&lt;int&gt; ret;\n        stack&lt;TreeNode*&gt; toTraversal;\n        while(root!&#x3D;NULL || !toTraversal.empty())&#123;\n            while(root !&#x3D; NULL)&#123;\n               toTraversal.push(root);\n               root &#x3D; root-&gt;left;\n            &#125;\n            root &#x3D; toTraversal.top();\n            toTraversal.pop();\n            ret.push_back(root-&gt;val);\n            root &#x3D; root-&gt;right;\n        &#125;\n        return ret;\n  &#125;\n&#125;;\n\n\n第二题\n第二题其实本来做过一个类似的，所以我直接用之前的递归思路写：\nclass Solution &#123;\npublic:\n    int climbStairs(int n) &#123;\n        int a;\n        if (n &#x3D;&#x3D; 1)\n            return 1;\n        else if (n &#x3D;&#x3D; 2)\n            return 2;\n        else &#123;\n            vector&lt;int&gt; a(n + 5, 0);\n            a[1] &#x3D; 1;\n            a[2] &#x3D; 2;\n            int i;\n            for (i &#x3D; 3;i &lt;&#x3D; n;i++)\n            &#123;\n                a[i] &#x3D; a[i - 1] + a[i - 2];\n            &#125;\n            return a[n];\n        &#125;\n    &#125;\n&#125;;\n\n但你会神奇的发现它超时了\n\n所以总结一下，如果直接用递归，时间复杂度回收O(n2)，我们需要换一下，如果你用递归打一下表，会发现它每一阶都是前面两阶的和，这样就可以使用动态规划的方法，使时间复杂度降到O(n)了\nclass Solution &#123;\npublic:\n    int climbStairs(int n) &#123;\n        int a;\n        if (n &#x3D;&#x3D; 1)\n            return 1;\n        else if (n &#x3D;&#x3D; 2)\n            return 2;\n        else &#123;\n            vector&lt;int&gt; a(n + 5, 0);\n            a[1] &#x3D; 1;\n            a[2] &#x3D; 2;\n            int i;\n            for (i &#x3D; 3;i &lt;&#x3D; n;i++)\n            &#123;\n                a[i] &#x3D; a[i - 1] + a[i - 2];\n            &#125;\n            return a[n];\n        &#125;\n    &#125;\n&#125;;\n\n\n第三题\n通过本题，不难发现其实是一个字符串识别的问题，并不是特别复杂，只要考虑清楚位置问题用switch就能很好的解决\nclass Solution &#123;\npublic:\n    int romanToInt(string s) &#123;\n       int count &#x3D; 0, t &#x3D; 0, flag &#x3D; 0;\t&#x2F;&#x2F;分别用于计数和标记\n       int i;\n       for (i &#x3D; s.length();i &gt;&#x3D; 0;i--)\n       &#123;\n        switch(s[i]) &#123;\t\t&#x2F;&#x2F;获得其中的单个字符\n        \tcase &#39;I&#39;: \n        \t\tt &#x3D; 1;\n        \t\tif(flag &#x3D;&#x3D; 1) &#123;\t\t&#x2F;&#x2F;判断是否在V的左面，如果在，则需减去\n        \t\t\tt &#x3D; -t; \t\n        \t\t\tflag &#x3D; 0;\n        \t\t&#125;\n        \t\tbreak;\n        \tcase &#39;V&#39;: flag &#x3D; 1; t &#x3D; 5; break;\n        \tcase &#39;X&#39;: \n        \t\tt &#x3D; 10;\n        \t\tif(flag &#x3D;&#x3D; 2) &#123;\n        \t\t\tt &#x3D; -t;\n        \t\t\tflag &#x3D; 0;\n        \t\t&#125;\n        \t\tflag &#x3D; 1; \n        \t\tbreak;\n        \tcase &#39;L&#39;: flag &#x3D; 2; t &#x3D; 50; break;\n        \tcase &#39;C&#39;: \n        \t\tt &#x3D; 100;\n        \t\tif(flag &#x3D;&#x3D; 3) &#123;\n        \t\t\tt &#x3D; -t;\n        \t\t\tflag &#x3D; 0;\n        \t\t&#125;\n        \t\tflag &#x3D; 2; \n        \t\tbreak;\n        \tcase &#39;D&#39;: flag &#x3D; 3; t &#x3D; 500; break;\n        \tcase &#39;M&#39;: flag &#x3D; 3; t &#x3D; 1000; break;\n        \t&#125;\n        \tcount +&#x3D; t;\n        &#125;\n       return count;\n    &#125;\n&#125;;\n\n图片](55566/8.png)\n","categories":["实验室相关"],"tags":["算法","实验室"]},{"title":"并不是0基础的python爬虫教学（一）","url":"/posts/64752/","content":"特别注意，本次教学是已经默认你已经拥有一定的python语法基础和HTML知识的前提下进行的！！！首先，我默认各位已经安装了pycharm环境，如果安装了anaconda更好，没有安装问题也不大，可以点击这里来进行环境的安装，现在让我们写出你人生中第一个爬虫程序吧！！！！\n要爬的网站豆瓣TOP250我们这次要爬的就是这么一个界面\n\n准确来说是这个界面的HTML代码，可以打开F12进行查看\n\n现在，我们打开pycharm，新建文件（我相信各位都会），导入requests包，如果用的anaconda的同学会发现已经自带了，如果没有的话可以自行搜索如何去安装,并在url中储存豆瓣TOP250的链接\nimport requests\n\nurl = 'https://movie.douban.com/top250'\n\n之后，我们在返回页面，检查网页源代码，发现它是已GET方式进行传输的\n\n这时候，我们就可以通过resquest来获取网页源代码\nimport requests\n\nurl = 'https://movie.douban.com/top250'\n\nresponse = requests.get(url = url)\n\n这时候的response还不能被我们所看懂，需要转换成text才可以，之后再输出\nimport requests\n\nurl = 'https://movie.douban.com/top250'\n\nresponse = requests.get(url = url)\nhtml = response.text\nprint(html)\n\n这时候查看输出\n\n这时候你会发现，它什么也没有输出，这时候，我们就要了解一下resquest方法了，如果可以了解它，各位就能破解很多一部分反爬问题了，它不仅需要网页链接，还需要模拟计算机登录的参数以及GET所需的参数（本文暂不涉及），而我们该如何模拟计算机登陆呢？\n这时候我们需要再次打开浏览器检查代码\n\n发现右下角有一个User-Agent，这个就是我们每次登录网页时计算机所提交的参数，用于证明我们是人为使用浏览器登陆的，如果不加，网页会很快识别我们为爬虫将我们拦截，所以我们要以字典的方式将它写入resquest中\nimport requests\n\nurl = 'https://movie.douban.com/top250'\nhead = &#123;\n    \"User-Agent\": \"Mozilla / 5.0(Windows NT 10.0;    Win64;    x64) AppleWebKit / 537.36(KHTML, like   Gecko) Chrome / 92.0    .4515    .107    Safari / 537.36\"&#125;\nresponse = requests.get(url = url, headers = head)\nhtml = response.text\nprint(html)\n\n再一次运行程序\n\n发现这次成功返回我们想要的第一页网页源代码了。\n如果各位成功了，那么恭喜，各位的第一个爬虫程序已经成功面世了。\n一个各位可能遇见的问题如果各位程序没有错误，但又报以下的错误\n\nValueError: check_hostname requires server_hostname\n那么可各位在运行时开着网络代理（俗称梯子），只要关闭代理，就可以正常使用了\n如果各位遇到问题，可以通过加我微信或者B站私信问我，我可以解决的一定会尽力帮助大家的。\n","categories":["python爬虫教学"],"tags":["python","教程"]}]